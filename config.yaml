global:
  # The topic used across prompts, this should be short and sweet
  topic: >
    Tool Use, the AI podcast
  # The overview of what your product/topic is, give as much detail as you'd like in here
  overview: >
    Tool Use is a weekly AI conversation where we talk about AI tools you can use in your life today. It's a podcast on youtube and other major podcast providers. The goal is to get more people to subscribe on youtube, and to get people listening on spotify. The two hosts are Ty Fiero and Mike Bird.

  # General settings
  enable_infinite_mode: true # Run pipeline infinitely
  wait_time_between_runs: 86400 # If infinite mode is on, it will wait this long before running the pipeline again  (this is 24 hours in seconds)
  continue_conversation_for_n_messages: 1 # Set to more than 1 if you want the conversation to continue on artificially before submitting

# ========================================
# Model configuration
# ========================================
model_tiers:
  tier_order: ["gemini_models", "openai_mini_models", "openai_large_models"] # The order matters, the first listed runs first. Remove a tier if you don't want to use it, or set enabled to false
  openai_mini_models:
    enabled: true
    provider: "openai"
    architect_agent: "openai/gpt-4o-mini"
    user_agent: "openai/gpt-4o-mini"
    system_agent: "openai/gpt-4o-mini"
    submission_model: "openai/gpt-4o-mini"
    daily_token_limit: 1000000 # Defaults to 1,000,000 tokens a day.
    env_key: "DATA_SHARING_OPENAI_API_KEY"
    submission_env_key: "DATA_SHARING_OPENAI_API_KEY"

  openai_large_models:
    enabled: true
    provider: "openai"
    architect_agent: "openai/gpt-4o"
    user_agent: "openai/gpt-4o"
    system_agent: "openai/gpt-4o"
    submission_model: "openai/gpt-4o"
    daily_token_limit: 100000 # Set to 100,000 by default. Increase if you want more conversations from the big models
    env_key: "DATA_SHARING_OPENAI_API_KEY"
    submission_env_key: "DATA_SHARING_OPENAI_API_KEY"

  gemini_models:
    enabled: true
    provider: "gemini"
    architect_agent: "gemini/gemini-2.0-flash"
    user_agent: "gemini/gemini-2.0-flash"
    system_agent: "gemini/gemini-2.0-flash"
    submission_model: "gemini/gemini-2.0-flash"
    daily_request_limit: 1500
    requests_per_minute: 15
    env_key: "DATA_SHARING_GEMINI_API_KEY"
    submission_env_key: "DATA_SHARING_GEMINI_API_KEY"

# ========================================
# Token Tracking configuration
# ========================================
token_tracking:
  tracking_file: "./logs/token_usage.json"
  stop_at_daily_token_limit: true
  auto_switch: true
  timezone: "UTC"
  min_tokens_percent: 1 # Stops when this percentage of the daily tokens remains

# ========================================
# Eval configuration
# ========================================
evaluation:
  enabled: true
  model: "openai/gpt-4o-mini"
  env_key: "DATA_SHARING_OPENAI_API_KEY"
  cutoff_score: 85

# ========================================
# Vector DB configuration
# ========================================
vector_db:
  enabled: true
  chunk_size: 600
  chunk_overlap_percentage: 15
  n_results: 3 # number of results to return per query
  similarity_threshold: 0.7
  index_name: "context_collection"
  embedding_api_key: "DATA_SHARING_OPENAI_API_KEY"
  embedding_model: "text-embedding-ada-002"
  persist_directory: "./chroma_db"

# ========================================
# Logging Configuration
# ========================================
logging:
  log_level: "DEBUG"
  log_file: "./logs/pipeline.log"
  rotation_size: "100 MB"
  retention_days: 1000
  compression: "zip"

  # ========================================
  # Paths for context and conversation artifacts, you shouldn't have to mess with this
  # ========================================
directories:
  context_folder: "./context/" # Folder containing markdown documentation & guidelines
  unprocessed_folder: "./conversations/unprocessed/"
  curated_folder: "./conversations/curated/"
  processed_folder: "./conversations/processed/"
